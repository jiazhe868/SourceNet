# SourceNet: Physics-Informed Deep Learning for Moment Tensor Inversion

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
![Build Status](https://img.shields.io/badge/build-passing-brightgreen)

**SourceNet** is a state-of-the-art deep learning framework designed to invert earthquake **Moment Tensors (MT)** and **Magnitude ($M_w$)** directly from single-station waveforms.

By combining a **Siamese 1D-ResNet** for local feature extraction with a **Transformer Encoder** for global event aggregation, SourceNet solves the geometric ambiguity problem inherent in traditional inversion methods. It uniquely integrates high-performance legacy **Fortran** code for physics calculations with modern **PyTorch** deep learning workflows.

---

## üåü Key Features

*   **Hybrid Computing**: Seamless integration of PyTorch tensors and Fortran-based physics kernels (via `ctypes`) for rigorous Kagan angle evaluation.
*   **Permutation Invariance**: Transformer-based aggregation allows processing variable numbers of seismic stations dynamically (Dynamic Batching).
*   **Unified Data Pipeline**: A single, robust `SeismicDataset` handling both Synthetic (Pretraining) and Real (Finetuning) data with on-the-fly augmentation.
*   **Experiment Management**: Fully configurable experiments using **Hydra**, supporting rapid switching between pretraining and finetuning modes without code changes.
*   **Production Ready**: Packaged as a standard Python library (`src` layout) with CI/CD-ready Makefiles and unit tests.

---

## üõ†Ô∏è Project Structure

The project follows a modern `src`-layout for better packaging and testing isolation.

```text
SourceNet/
‚îú‚îÄ‚îÄ Makefile                 # Automation entry points (build, test, clean)
‚îú‚îÄ‚îÄ pyproject.toml           # Dependency and package management
‚îú‚îÄ‚îÄ configs/                 # Hydra configuration center
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml          # Global defaults
‚îÇ   ‚îú‚îÄ‚îÄ model/               # Network architecture params
‚îÇ   ‚îú‚îÄ‚îÄ data/                # Data paths and augmentation settings
‚îÇ   ‚îî‚îÄ‚îÄ training/            # LR, Loss functions, Epochs
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ sourcenet/           # Core Python package
‚îÇ       ‚îú‚îÄ‚îÄ models/          # Neural Network definitions (SourceNet)
‚îÇ       ‚îú‚îÄ‚îÄ data/            # Unified Dataset & Collate functions
‚îÇ       ‚îú‚îÄ‚îÄ utils/           # Physics (Beachballs) & Metrics (Focal Loss)
‚îÇ       ‚îî‚îÄ‚îÄ ext/             # Fortran extensions (mtdcmp.f)
‚îú‚îÄ‚îÄ scripts/                 # Execution drivers
‚îÇ   ‚îú‚îÄ‚îÄ train.py             # Unified Training/Finetuning script
‚îÇ   ‚îî‚îÄ‚îÄ inference.py         # Evaluation & Visualization
‚îî‚îÄ‚îÄ tests/                   # Unit and Integration tests
```

---

## üöÄ Installation

### Prerequisites
*   Python >= 3.9
*   GFortran (for compiling physics extensions)
*   CUDA Toolkit (optional, for GPU training)

### Steps

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/yourusername/SourceNet.git
    cd SourceNet
    ```

2.  **Create a virtual environment (Recommended):**
    ```bash
    conda create -n sourcenet python=3.9
    conda activate sourcenet
    ```

3.  **Build and Install:**
    Use the `Makefile` to compile the Fortran extensions and install the package in editable mode.
    ```bash
    make build    # Compiles src/sourcenet/ext/mtdcmp.f -> mtdcmp.so
    make install  # Installs python dependencies via pip
    ```

4.  **Verify Installation:**
    Run the unit tests to ensure physics kernels and neural networks are functioning correctly.
    ```bash
    make test
    ```

---

## üèÉ Usage

SourceNet uses **Hydra** for configuration management. You can override any parameter from the command line.

### 1. Pretraining (Synthetic Data)

By default, the system runs in pretraining mode using MSE loss and high learning rates.

```bash
python scripts/train.py data=synthetic training=pretrain
```

### 2. Finetuning (Real Data)

Switch to Transfer Learning mode (Focal Loss, Differential Learning Rates) by simply changing the config group.

```bash
python scripts/train.py \
    data=real_socal \
    training=finetune \
    training.pretrained_ckpt=/path/to/best_sourcenet_pretrain.pth
```

### 3. Inference & Visualization

Run evaluation on the test set to generate scatter plots and beachball comparisons.

> **Note:** If `model_path` is not defined in your `config.yaml`, use `+model_path` to append it dynamically.

```bash
python scripts/inference.py \
    data=real_socal \
    +model_path=outputs/debug_finetune/best_finetuned_model.pth \
    device='cuda'
```

**Output Artifacts (saved in `outputs/YYYY-MM-DD/HH-MM-SS/`):**
*   `scatter_results.pdf`: Comparison of predicted vs true MT components.
*   `beachball_grid.pdf`: Side-by-side comparison of focal mechanisms with Kagan angle annotation.
*   `kagan_histogram.pdf`: Distribution of angular errors.

---

## ‚öôÔ∏è Configuration

Hyperparameters are managed in `configs/`. Key files:

*   **`configs/model/sourcenet.yaml`**:
    *   `embed_dim`: Dimension of station embeddings (Default: 128).
    *   `layers`: Number of Transformer layers (Default: 3).
*   **`configs/training/finetune.yaml`**:
    *   `lr_backbone`: Learning rate for the CNN encoder (Default: 2e-6).
    *   `lr_head`: Learning rate for regression heads (Default: 2e-6).
    *   `focal_gamma`: Focusing parameter for robust regression (Default: 1.5).

To change the batch size dynamically:
```bash
python scripts/train.py data.batch_size=128
```

---

## üß™ Development

We adhere to strict engineering standards.

*   **Linting**: Code is formatted using `black` and imports sorted by `isort`.
*   **Testing**: All core logic (Physics, Dataset, Model) is covered by `pytest`.

To run the full test suite during development:
```bash
make test
```

To clean build artifacts:
```bash
make clean
```

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

